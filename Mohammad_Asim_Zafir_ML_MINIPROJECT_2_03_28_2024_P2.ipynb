{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mzafir/aps/blob/master/Mohammad_Asim_Zafir_ML_MINIPROJECT_2_03_28_2024_P2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "60f5rOA4JVO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOAD THE FACEBOOK_COMBINED.TXT INTO THE RUNTIME BEFORE RUNNING THIS PROJECT."
      ],
      "metadata": {
        "id": "a6wwz8XqJSFI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kI8YneAMxkWw"
      },
      "outputs": [],
      "source": [
        "!pip install networkx pandas numpy scikit-learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Steps**\n",
        "1. Load The Graph Data\n",
        "2. Generate the Node features ( Understand what each Node does)\n",
        "3. Check for missing values, any duplicate rows ? (Scale?)\n",
        "4. Plot historgram for each feature\n",
        "5. Plot scatter plots between each feature. Any insights?\n",
        "6. Test Kmeans ( Use Elbow Method to find a good K)\n",
        "7. Test DBSCAN\n",
        "8. Test GMM\n",
        "9. Analyze and Validate  \n",
        "Plot graphs visualzing the different clusters\n"
      ],
      "metadata": {
        "id": "B2HDZ9-Z9wGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "# Load The Graph\n",
        "\n",
        "G = nx.read_edgelist(\"facebook_combined.txt\",create_using=nx.Graph(),nodetype=int)\n",
        "\n",
        "print(f\"Numer of Edges : {G.number_of_edges()}\")\n",
        "print(f\"Number of Nodes : {G.number_of_nodes()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_s-7BzayO7m",
        "outputId": "5fb14ac6-53b9-428e-9e66-caab1699ea4f"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numer of Edges : 88234\n",
            "Number of Nodes : 4039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import random\n",
        "\n",
        "# Assuming 'G' is your original graph with 4000 nodes\n",
        "G = nx.generators.random_graphs.erdos_renyi_graph(4000, 0.01)\n",
        "\n",
        "# Randomly sample 400 nodes\n",
        "sampled_nodes = random.sample(G.nodes(), 400)\n",
        "\n",
        "# Create a subgraph with the sampled nodes\n",
        "subG = G.subgraph(sampled_nodes)\n",
        "\n",
        "# Now, 'subG' is your smaller graph with approximately 400 nodes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pf_vxxaJvlpH",
        "outputId": "537a1a68-b943-48b9-aedd-c0170071f86d"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-99-75a09fb5d1aa>:8: DeprecationWarning: Sampling from a set deprecated\n",
            "since Python 3.9 and will be removed in a subsequent version.\n",
            "  sampled_nodes = random.sample(G.nodes(), 400)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: Initiliazing the networkx library and reading the dataset into the Graph object \"G\".\n",
        "\n",
        "In this step - identify number of edges inbound to a given Node and also identify total number of edges as well as nodes\n"
      ],
      "metadata": {
        "id": "6V8L58ozxkuD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA AND FEATURE ENGINEERING"
      ],
      "metadata": {
        "id": "Qr3_UVgw_0ZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Node Degree\n",
        "degree_dict = dict(G.degree())\n",
        "\n",
        "#Clustering Coefficient\n",
        "clustering_dict = dict(nx.clustering(G))\n",
        "\n",
        "# EigenVector Centrality\n",
        "eigen_vector_centrality = nx.eigenvector_centrality(G)\n",
        "\n",
        "#Average Neighbor Degree\n",
        "average_neighbor_degree_dict = nx.average_neighbor_degree(G)\n",
        "\n",
        "#Degree Centrality\n",
        "degree_centrality_dict = nx.degree_centrality(G)\n",
        "\n",
        "#Eccentricity\n",
        "eccentricity_dict = nx.eccentricity(G)\n",
        "\n",
        "#Closeness Centrality\n",
        "closeness_centrality_dict = nx.closeness_centrality(G)\n",
        "\n",
        "#Betweeness Centrality\n",
        "betweenness_centrality_dict = nx.betweenness_centrality(G)\n",
        "\n"
      ],
      "metadata": {
        "id": "wxStL84nzG3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Degree: This is the most basic and intuitive measure of a node's connectivity; it counts how many connections (edges) a node has.\n",
        "#In undirected graphs, it simply counts all the edges attached to a node. In directed graphs, you can have in-degree and out-degree, counting incoming and outgoing connections separately.\n",
        "\n",
        "# Clustering Coefficient: This measures the degree to which nodes in a graph tend to cluster together. Specifically, for a given node, it represents the likelihood that its neighbors are also connected with each other. It's a measure of the local group cohesiveness around a node.\n",
        "\n",
        "# Average Neighbor Degree: This metric calculates the average degree of all the neighbors of each node. It provides insight into the connectivity of a node's immediate network, indicating whether a node is surrounded by highly connected or sparsely connected nodes.\n",
        "\n",
        "# Degree Centrality: Similar to the degree but normalized to lie between 0 and 1, degree centrality measures a node's relative importance based on the number of connections it has to other nodes in the network. It's calculated as the degree of the node divided by the maximum possible degree in the graph.\n",
        "\n",
        "# Eccentricity: This metric measures the greatest distance between a node and all other nodes in the graph. In other words, it's the maximum number of edges that need to be traversed to reach the furthest node in the graph from the given node. Nodes with low eccentricity are more central because they are closer to all other nodes.\n",
        "\n",
        "# Closeness Centrality: This metric measures how close a node is to all other nodes in the network, calculated as the reciprocal of the sum of the shortest path distances from a node to all other nodes. It reflects how easily a node can reach all other nodes, with higher values indicating shorter paths to all others, implying a more central position in the network.\n",
        "\n",
        "# Betweenness Centrality: This measures the extent to which a node lies on the shortest paths between other nodes. It quantifies the number of times a node acts as a bridge along the shortest path between two other nodes. This metric identifies nodes that serve as critical connectors or 'bottlenecks' in the network, through which a large amount of network traffic can flow.\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "features_df = pd.DataFrame({\n",
        "    \"Degree\" : pd.Series(degree_dict),\n",
        "    \"Clustering_Coefficient\" : pd.Series(clustering_dict),\n",
        "    \"EigenVector_Centrality\" : pd.Series(eigen_vector_centrality),\n",
        "    \"Average Neighbor Degree\": pd.Series(average_neighbor_degree_dict),\n",
        "    \"Degree Centrality\"     : pd.Series(degree_centrality_dict),\n",
        "    \"Eccentricity\"          : pd.Series(eccentricity_dict),\n",
        "    \"Closeness Centrality\"   : pd.Series(closeness_centrality_dict),\n",
        "    \"Betweenness Centrality\" : pd.Series(betweenness_centrality_dict),\n",
        "})\n",
        "\n"
      ],
      "metadata": {
        "id": "YdwqYmyyzxl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_df.head()"
      ],
      "metadata": {
        "id": "6jcs4npa2IYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4zMTw8JK30de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "features_df.describe()\n",
        "\n",
        "#checking for missing values and analyzing the dataset"
      ],
      "metadata": {
        "id": "z4QFjbuj8Gux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#NON SCALED FEATURE DATAFRAME\n",
        "features_df"
      ],
      "metadata": {
        "id": "fmFgIyHn31lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SCALING METHODS USED\n",
        "# Min Max , Bound you data between 0 and 1 (x - x_min) / (x_max - x_min)\n",
        "# Standard Scaling . (X- mean / standard deviation)\n",
        "\n"
      ],
      "metadata": {
        "id": "VijLyYj-GMfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "features_scaled = scaler.fit_transform(features_df)\n",
        "features_scaled\n",
        "\n",
        "#standard scalar : this has a guassian distribution property and ideally all features are centered around mean=0 bounds: (-1,1)\n",
        "#min-max: bounds (0,1)"
      ],
      "metadata": {
        "id": "dq9MuFYM6lXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lLsKmYEV8Fz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_scaled = pd.DataFrame(features_scaled,columns=features_df.columns)\n",
        "features_scaled"
      ],
      "metadata": {
        "id": "FWHJy6s47yGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n"
      ],
      "metadata": {
        "id": "bFISlHky4cQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#normalized_df = (features_df - features_df.min()) / (features_df.max() - features_df.min())\n",
        "\n",
        "#chosed to moved on with standard scalar only but tried min / max as well."
      ],
      "metadata": {
        "id": "YpS0G19x4erD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_scaled = pd.DataFrame(features_scaled,columns=features_df.columns)\n",
        "features_scaled"
      ],
      "metadata": {
        "id": "j_ydQHTPeeNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET VISUALIZATION"
      ],
      "metadata": {
        "id": "vSUeiQiM_rpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting standardized  feature values\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(nrows=6, ncols=1, figsize=(8, 24))  # Adjust figsize as needed\n",
        "\n",
        "features = [\n",
        "    'EigenVector_Centrality',\n",
        "    'Average Neighbor Degree',\n",
        "    'Betweenness Centrality',\n",
        "    'Clustering_Coefficient',\n",
        "    'Degree Centrality',\n",
        "    'Closeness Centrality'\n",
        "]\n",
        "\n",
        "titles = [\n",
        "    'EigenVector Centrality',\n",
        "    'Average Neighbor Degree',\n",
        "    'Betweenness Centrality',\n",
        "    'Clustering Coefficient',\n",
        "    'Degree Centrality',\n",
        "    'Closeness Centrality'\n",
        "]\n",
        "\n",
        "for ax, feature, title in zip(axes.flatten(), features, titles):\n",
        "    if feature in features_scaled.columns:  # Check if the column exists\n",
        "        features_scaled[feature].plot(kind='hist', bins=50, title=title, ax=ax)\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2z6fQFTOJs0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_table=[]\n",
        "mean_table.append(features_scaled['Degree'].mean())\n",
        "mean_table.append(features_scaled['Average Neighbor Degree'].mean())\n",
        "mean_table.append(features_scaled['Betweenness Centrality'].mean())\n",
        "mean_table.append(features_scaled['Closeness Centrality'].mean())\n",
        "mean_table.append(features_scaled['Degree Centrality'].mean())\n",
        "mean_table.append(features_scaled['Eccentricity'].mean())\n",
        "mean_table.append(features_scaled['EigenVector_Centrality'].mean())\n"
      ],
      "metadata": {
        "id": "VhZDUMFo77vL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_table"
      ],
      "metadata": {
        "id": "kl9Lf0_QLJbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cuQR3PLTOHck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fulqkZUy1Q4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.pairplot(features_scaled)\n",
        "\n"
      ],
      "metadata": {
        "id": "eromsgOm3-rE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold_val=0.9\n",
        "\n",
        "correlation_matrix=features_scaled.corr()\n",
        "\n",
        "correlation_matrix\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "sns.heatmap(correlation_matrix,annot=True, cmap='coolwarm')\n",
        "plt.show()\n",
        "\n",
        "highly_correlated_features = set()\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i):\n",
        "        if abs(correlation_matrix.iloc[i, j]) > threshold_val:\n",
        "            colname = correlation_matrix.columns[i]\n",
        "            highly_correlated_features.add(colname)\n",
        "\n",
        "print(highly_correlated_features)\n"
      ],
      "metadata": {
        "id": "rhkyGFBrhDCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CLUSTERING METHODS- \"K-MEANS\"**"
      ],
      "metadata": {
        "id": "NOVTF-LQGuSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(3)\n",
        "\n",
        "features_df['Cluster'] = kmeans.fit_predict(features_df)\n",
        "\n"
      ],
      "metadata": {
        "id": "7UFoe5A62LGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_df['Cluster'].value_counts()"
      ],
      "metadata": {
        "id": "duL8HEG22PY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "wss = []\n",
        "for i in range(1,15):\n",
        "    kmeans = KMeans(n_clusters=i,random_state=42)\n",
        "    kmeans.fit(features_scaled)\n",
        "    wss.append(kmeans.inertia_)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(range(1,15),wss,marker='o')\n",
        "plt.title('Elbow Method')\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EmL2QdZB2t_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NOTE: The technique used in this method is by employing EBLOW METHOD, we identify number for cluster for Kmeans algorith by identifying the biggest intertia drop from the ELBOW plot. This number will be then used by the Kmean clustering algorithm. as it is event the relatively larger drop is between 3 to 5 - hence N value for Kmeans is taken as 3"
      ],
      "metadata": {
        "id": "JJDG8Liyw209"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5acZtyfuw14o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Relationship Visualization **"
      ],
      "metadata": {
        "id": "HnAz-XUMD942"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(3)\n",
        "\n",
        "features_scaled['Cluster'] = kmeans.fit_predict(features_scaled)\n"
      ],
      "metadata": {
        "id": "Y9PUhG3D_aI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(data=features_scaled,x='Degree',y='Clustering_Coefficient',hue=\"Cluster\")\n",
        "plt.title(\"Degree vs Clustering Coefficient\")\n",
        "plt.xlabel('Degree')\n",
        "plt.ylabel('Clustering Coefficient')\n",
        "plt.legend(title=\"Cluster\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dg2RBOJF_m3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XjKmxPG8C5Vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(data=features_scaled,x='Betweenness Centrality',y='Degree Centrality',hue=\"Cluster\")\n",
        "plt.title(\"Between Centrality  vs Degree Centrality\")\n",
        "plt.xlabel('Betweenness Centrality')\n",
        "plt.ylabel('Degree Centrality')\n",
        "plt.legend(title=\"Cluster\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PaJP4yH_C517"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLUSTER VALIDATION TECHNIQUES"
      ],
      "metadata": {
        "id": "5NrC4eSkEVQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "J1E-KgxzEVPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_labels = kmeans.labels_\n"
      ],
      "metadata": {
        "id": "vDwJaq9vMoSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "kmeans_score = silhouette_score(features_scaled, cluster_labels)\n",
        "kmeans_score"
      ],
      "metadata": {
        "id": "qR0MnWS8OcAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " silhouette score of 0.3085902950154001 suggests that, on average, the clusters are moderately well defined but not perfectly distinct."
      ],
      "metadata": {
        "id": "2i2wUpPJBIbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VEqAD00NBGtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import davies_bouldin_score\n",
        "\n",
        "db_index = davies_bouldin_score(features_scaled, cluster_labels)\n",
        "db_index"
      ],
      "metadata": {
        "id": "UCCBwwozOhEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Davies-Bouldin Index of 1.1365278449188165 indicates a moderate level of cluster compactness and separation. It suggests that there's room for improvement, but the clusters are not excessively overlapped or too spread out."
      ],
      "metadata": {
        "id": "nohjfUZTB5-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import calinski_harabasz_score\n",
        "\n",
        "ch_index = calinski_harabasz_score(features_scaled, cluster_labels)\n",
        "ch_index\n"
      ],
      "metadata": {
        "id": "9074_pyjOplq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Calinski-Harabasz score of 1280.9232666039757 is relatively high, suggesting that, on average,  clusters are quite compact and well-separated from each other."
      ],
      "metadata": {
        "id": "UMYVrpvBCPFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "tsne_results = tsne.fit_transform(features_scaled.drop('Cluster', axis=1))\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "sns.scatterplot(x=tsne_results[:, 0], y=tsne_results[:, 1], hue=features_scaled['Cluster'], palette='viridis', legend=\"full\")\n",
        "plt.title('t-SNE Visualization of Clusters')\n"
      ],
      "metadata": {
        "id": "C4FytfzdLfH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.decomposition import PCA\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# # Performing PCA to reduce to 2 dimensions for visualization\n",
        "# pca = PCA(n_components=2)\n",
        "# pca_result = pca.fit_transform(features_scaled)\n",
        "\n",
        "# # Adding the results back to your DataFrame for easy plotting\n",
        "# features_scaled['PCA1'] = pca_result[:, 0]\n",
        "# features_scaled['PCA2'] = pca_result[:, 1]\n",
        "\n",
        "# # Plotting the first two PCA components with clusters as color\n",
        "# plt.figure(figsize=(10, 8))\n",
        "# sns.scatterplot(x='PCA1', y='PCA2', hue='Cluster', data=features_scaled, palette='viridis', legend=\"full\", alpha=0.5)\n",
        "# plt.title('PCA - First two principal components')\n",
        "# plt.xlabel('Principal Component 1')\n",
        "# plt.ylabel('Principal Component 2')\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "jN-Bmu2lN1zN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U4c2G88Fg_lS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_scaled\n"
      ],
      "metadata": {
        "id": "18inFx456pAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tsne_results"
      ],
      "metadata": {
        "id": "4xPB9v98NKYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tsne = TSNE(n_components=3, random_state=42)\n",
        "tsne_results = tsne.fit_transform(features_scaled.drop('Cluster', axis=1))\n",
        "tsne_results"
      ],
      "metadata": {
        "id": "qVXqbGFhNO25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bpyVQVRrRGvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CLUSTERING METHOD - \"DBSCAN\"**"
      ],
      "metadata": {
        "id": "ASHHRbbbG_uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=2).fit(features_scaled)\n",
        "\n",
        "# Get the cluster labels (note: '-1' means outlier)\n",
        "labels = dbscan.labels_\n",
        "\n",
        "# Print cluster labels\n",
        "print(labels)\n"
      ],
      "metadata": {
        "id": "BuNcnZz-Wat1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Assuming 'features_scaled' is your scaled dataset and 'labels' are the labels from DBSCAN\n",
        "score_dbscan = silhouette_score(features_scaled, labels)\n",
        "\n",
        "print('Silhouette Score: %.3f' % score_dbscan)"
      ],
      "metadata": {
        "id": "wiPdgKRK-HPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import calinski_harabasz_score\n",
        "ch_score_dbscan = calinski_harabasz_score(features_scaled, labels)\n",
        "print(\"Calinski-Harabasz Score: \", ch_score_dbscan)"
      ],
      "metadata": {
        "id": "e5mNN8GxYJiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import davies_bouldin_score\n",
        "\n",
        "db_index_dbscan = davies_bouldin_score(features_scaled, labels)\n",
        "db_index_dbscan"
      ],
      "metadata": {
        "id": "S9FZTJcKYdxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pClEefK_GKnD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "silhouette score of 0.119 suggests that, on average, the clusters are moderately well defined but not perfectly distinct."
      ],
      "metadata": {
        "id": "xIE9by-sF07p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reduce the dimensionality to 2 components for visualization\n",
        "pca = PCA(n_components=2)\n",
        "reduced_features = pca.fit_transform(features_scaled)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=labels, cmap='viridis', s=50, alpha=0.6)\n",
        "plt.title(\"DBSCAN Clustering with PCA-reduced Data\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.colorbar(label='Cluster Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CzgSLwd2-V1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# t-SNE for dimensionality reduction to 2 dimensions\n",
        "tsne = TSNE(n_components=2, perplexity=30, n_iter=300)\n",
        "tsne_features = tsne.fit_transform(features_scaled)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(tsne_features[:, 0], tsne_features[:, 1], c=labels, cmap='viridis', s=50, alpha=0.6)\n",
        "plt.title(\"DBSCAN Clustering with t-SNE-reduced Data\")\n",
        "plt.xlabel(\"t-SNE Feature 1\")\n",
        "plt.ylabel(\"t-SNE Feature 2\")\n",
        "plt.colorbar(label='Cluster Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "daq0ri01-gH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.datasets import make_blobs  # Used here for generating example data\n",
        "\n",
        "\n",
        "# Number of clusters\n",
        "n_clusters = 4\n",
        "\n",
        "gmm = GaussianMixture(n_components=n_clusters, random_state=0)\n",
        "\n",
        "# Fitting the model and predicting the clusters\n",
        "gmm_labels = gmm.fit_predict(features_scaled)\n",
        "\n",
        "# Plotting the clusters\n",
        "plt.scatter(features_scaled.iloc[:, 0], features_scaled.iloc[:, 1], c=labels, cmap='viridis', s=40)\n",
        "plt.title(\"Clusters identified by GMM\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aNeLcJFAKdPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "centers = gmm.means_\n",
        "print(\"Cluster Centers:\\n\", centers)\n"
      ],
      "metadata": {
        "id": "JxyhZ3oOPzQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KSwX_CsfAHBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "gmm_score = silhouette_score(features_scaled, gmm_labels)\n",
        "print(\"Silhouette Score: \", gmm_score)\n"
      ],
      "metadata": {
        "id": "Fv_483xgP6cY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import calinski_harabasz_score\n",
        "ch_score_gmm = calinski_harabasz_score(features_scaled, gmm_labels)\n",
        "print(\"Calinski-Harabasz Score: \", ch_score_gmm)\n"
      ],
      "metadata": {
        "id": "8HWtD4RBQF_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import davies_bouldin_score\n",
        "\n",
        "db_index_gmm = davies_bouldin_score(features_scaled, gmm_labels)\n",
        "db_index_gmm"
      ],
      "metadata": {
        "id": "npINgQ_aUh5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Kmeans Sillhoutscore\", kmeans_score)\n",
        "print(\"DBScan Sillhoutscore\", score_dbscan)\n",
        "print(\"GMM Sillhoutescore\", gmm_score)\n",
        "print(\"\\n\")\n",
        "print(\"Kmeans-Calinski-Harabasz-Score\", ch_index)\n",
        "print(\"DBScan-Calinski-Harabasz-Score\", ch_score_dbscan)\n",
        "print(\"GMM-Calinski-Harabasz-Score\", ch_score_gmm)\n",
        "print(\"\\n\")\n",
        "print(\"Kmeans-davies_bouldin_score\", db_index)\n",
        "print(\"DBScan-davies_bouldin_score\", db_index_dbscan)\n",
        "print(\"GMM-davies_bouldin_score\", db_index_gmm)\n",
        "\n"
      ],
      "metadata": {
        "id": "Se5iALoHUtAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANALYSIS ON CLUSTER EVAUTION METRIC\n",
        "\n",
        "\n",
        "**DBSCAN** seems to perform the best among all  three methods based on these metrics. This indicates it is able to find more meaningful, well-separated clusters for your dataset. This might be due to its ability to handle noise and identify clusters of arbitrary shape. *FAST*\n",
        "\n",
        "\n",
        "**K-means** shows moderate performance, suggesting it can still find some structure in the data but might be limited by its assumption of spherical clusters. *RELATIVELY SLOW*\n",
        "\n",
        "\n",
        "**GMM** appears to struggle the most with your dataset according to these metrics. This could be due to various reasons, such as the choice of the number of components, the complexity of the data, or the presence of noise. *FAST*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xLyoVehSa5UX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "features_scaled['Cluster'] = labels  # Assuming features_scaled is a DataFrame\n",
        "cluster_summary = features_scaled.groupby('Cluster').mean()\n",
        "print(cluster_summary)\n"
      ],
      "metadata": {
        "id": "EvdfZcWUQLTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.boxplot(x='Cluster', y='Degree Centrality', data=features_scaled)\n"
      ],
      "metadata": {
        "id": "qlJcJ-qIQSvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install networkx python-louvain\n",
        "\n"
      ],
      "metadata": {
        "id": "EsbwKzY8k604"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RUNNING COMMUNITY BASED ALGORITHM - GIRVAN NEWMAN"
      ],
      "metadata": {
        "id": "SnmgP0wdurOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "from networkx.algorithms.community import girvan_newman\n",
        "\n",
        "\n",
        "\n",
        "#  Girvan-Newman algorithm\n",
        "communities = girvan_newman(subG)\n",
        "\n",
        "#  access the communities\n",
        "community_list = list(communities)\n",
        "\n",
        "# first level of communities\n",
        "print(community_list[0])\n"
      ],
      "metadata": {
        "id": "i5dMMhValExi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRAPH SAMPLING"
      ],
      "metadata": {
        "id": "O3NVQ0YXu093"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import networkx as nx\n",
        "# import random\n",
        "\n",
        "# def random_node_sampling(G, fraction=0.1):\n",
        "#     sampled_nodes = random.sample(G.nodes(), int(len(G.nodes()) * fraction))\n",
        "#     return G.subgraph(sampled_nodes)\n",
        "\n",
        "# def bfs_sampling(G, start_node, depth=2):\n",
        "#     nodes = set([start_node])\n",
        "#     nodes.update(nx.single_source_shortest_path_length(G, start_node, depth).keys())\n",
        "#     return G.subgraph(nodes)\n"
      ],
      "metadata": {
        "id": "2NAYXm-tuVHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRAPH PARTITIONING"
      ],
      "metadata": {
        "id": "t11LnsI2u5EH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Assuming the use of a library like METIS for partitioning\n",
        "# import metis\n",
        "# import networkx as nx\n",
        "\n",
        "# def partition_graph(G, num_partitions=4):\n",
        "#     _, parts = metis.part_graph(G, num_partitions)\n",
        "#     partitions = [[] for _ in range(num_partitions)]\n",
        "#     for i, p in enumerate(parts):\n",
        "#         partitions[p].append(i)\n",
        "#     return partitions\n"
      ],
      "metadata": {
        "id": "cyfl7QwkuYXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PARALLEL PROCESSING"
      ],
      "metadata": {
        "id": "1uPpnNRqu8Q8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from joblib import Parallel, delayed\n",
        "# import networkx as nx\n",
        "\n",
        "# # Example: Parallel computation of centrality measures\n",
        "# def compute_centrality(subgraph):\n",
        "#     return nx.betweenness_centrality(subgraph)\n",
        "\n",
        "# def parallel_centrality_computation(G, partitions):\n",
        "#     subgraphs = [G.subgraph(nodes) for nodes in partitions]\n",
        "#     results = Parallel(n_jobs=-1)(delayed(compute_centrality)(sg) for sg in subgraphs)\n",
        "#     return results\n"
      ],
      "metadata": {
        "id": "yOyHvCxduckq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EetHXzF38q82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STATISTICAL ANALYSIS"
      ],
      "metadata": {
        "id": "oeehCgHW9VSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy scipy sklearn matplotlib seaborn\n"
      ],
      "metadata": {
        "id": "GEx7sc4R6BL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Example DataFrame setup\n",
        "# df = pd.DataFrame(data, columns=['Feature1', 'Feature2', ..., 'Cluster'])\n"
      ],
      "metadata": {
        "id": "2t2oBqFV6CXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "# calculate average pairwise distance within clusters\n",
        "def avg_pairwise_distance(features_scaled, cluster_col='Cluster'):\n",
        "    cluster_distances = {}\n",
        "    for cluster in features_scaled[cluster_col].unique():\n",
        "        cluster_data = features_scaled[features_scaled[cluster_col] == cluster].drop(columns=[cluster_col])\n",
        "        distances = pdist(cluster_data, metric='euclidean')\n",
        "        avg_distance = np.mean(distances)\n",
        "        cluster_distances[cluster] = avg_distance\n",
        "    return cluster_distances\n",
        "\n",
        "# print average pairwise distances\n",
        "cluster_distances = avg_pairwise_distance(features_scaled, 'Cluster')\n",
        "print(cluster_distances)\n"
      ],
      "metadata": {
        "id": "Wv0-wMGp6CNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***STATISTICAL SUMMARY PER CLUSTER ***"
      ],
      "metadata": {
        "id": "tVDfg-Ia9GLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# statistical summaries for each cluster\n",
        "cluster_summaries = features_scaled.groupby('Cluster').describe()\n",
        "print(cluster_summaries)\n"
      ],
      "metadata": {
        "id": "gHhumu2m82G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Example feature to visualize\n",
        "feature_to_visualize = 'EigenVector_Centrality'\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.boxplot(x='Cluster', y=feature_to_visualize, data=features_scaled)\n",
        "plt.title(f'Distribution of {feature_to_visualize} Across Clusters')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lmyvmdWN9fQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AnIPGiAa-P_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Example feature to visualize\n",
        "feature_to_visualize = 'Degree'\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.boxplot(x='Cluster', y=feature_to_visualize, data=features_scaled)\n",
        "plt.title(f'Distribution of {feature_to_visualize} Across Clusters')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2PHNyPF9-QQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#feature to visualize\n",
        "feature_to_visualize = 'Degree Centrality'\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.boxplot(x='Cluster', y=feature_to_visualize, data=features_scaled)\n",
        "plt.title(f'Distribution of {feature_to_visualize} Across Clusters')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mbQp1Z76-VNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Example feature to visualize\n",
        "feature_to_visualize = 'Eccentricity'\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.boxplot(x='Cluster', y=feature_to_visualize, data=features_scaled)\n",
        "plt.title(f'Distribution of {feature_to_visualize} Across Clusters')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CUIGqI5u-uw2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}