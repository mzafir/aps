{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO9Bp4jS09AqCz0ZBuiUnD8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mzafir/aps/blob/master/image-detection-analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1vMp16TImqA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PROJECT -2 COHESIVE GROUP EMOTIONS RECOGNITION"
      ],
      "metadata": {
        "id": "d68mb884nyzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Motivation\n",
        "Recognizing and understanding emotions in interpersonal communication is pivotal for deciphering social dynamics and improving various aspects of human interaction. Emotions are conveyed through multiple channels, including facial expressions, vocal intonations, and body language. The ability to discern and interpret emotions in group settings provides valuable insights into collective emotional states, which can inform our understanding of group satisfaction, engagement, emotional contagion, and conflict resolution.\n",
        "\n",
        "\n",
        "\n",
        "Recent technological advancements have created a growing demand for emotion recognition, with applications spanning human-computer interfaces, healthcare for assessing emotional well-being, and security systems for identifying suspicious behaviors. Facial expressions, in particular, offer a rich source of dynamic information and are readily analyzable, thanks to the availability of large-scale emotion recognition datasets and sophisticated deep-learning models.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Problem Statement\n",
        "Current approaches to group-level emotion recognition often treat groups as homogenous entities, failing to capture the diversity of emotions within a group. These methods typically extract features from entire group images, limiting their precision and granularity. Some previous approaches have attempted to address this issue by employing combinations of convolutional neural networks (CNNs) and long short-term memory networks (LSTMs) to extract features from both whole images and facial regions or fusing results from individually trained CNNs on faces and whole images. While these approaches show promise, there remains room for improvement in recognizing individual emotions within groups accurately.\n",
        "\n",
        "\n",
        "\n",
        "To address these limitations, the objective is to propose an innovative solution that focuses on individualizing emotion recognition within groups using advanced deep learning techniques, including face detection and pre-trained emotion recognition models. This approach aims to break down group images, acknowledge the unique emotional expressions of each individual, and improve the accuracy and nuance of group-level emotion recognition, addressing an aspect that previous approaches still need to explore adequately.\n",
        "\n",
        "\n",
        "\n",
        "The primary goal is to showcase the power of integration and adaptation of existing technologies for cohesive group recognition. Unlike traditional approaches that require training custom models, our focus is on creating an innovative pipeline by extending and adapting existing frameworks.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Key Emphasis\n",
        "Leveraging existing frameworks: The foundation of this project is built upon existing technologies, including pre-trained models for face detection and individual facial expression recognition. There is no need for time-consuming model fine-tuning or extensive data collection and training phases. The challenge lies in extending these frameworks to effectively work with group images.\n",
        "\n",
        "\n",
        "Scalability: By focusing on existing technologies, our approach is highly scalable. Learners can apply this pipeline to various applications without the need for extensive model retraining, making it versatile and adaptable.\n",
        "\n",
        "\n",
        "The technology stack used to build the cohesive group recognition prototype is not fixed, and there are no restrictions in terms of which technologies to employ. Instead, we encourage you to approach this project with an open mind. Ultimately, the goal is to foster a spirit of innovation, creativity, and adaptability. By being open to a wide array of technologies and staying flexible in your approach, you can craft a prototype that showcases the potential of cohesive group recognition in your specific/generic application domain(s).\n",
        "\n",
        "\n",
        "\n",
        "Objectives\n",
        "Develop a framework capable of accurately recognizing individual emotions within group images/videos.\n",
        "\n",
        "\n",
        "Utilize cutting-edge face detection techniques such as YOLOvx, HaarCascade, SSD, etc. to identify and locate individual faces within group images.\n",
        "\n",
        "\n",
        "Employ pre-trained DeepFace and FER models to predict distinct emotions for each individual featured in the image.\n",
        "\n",
        "\n",
        "Label some images manually and evaluate the proposed model using metrics such as cross-entropy losses among emotions and sample analyses.\n",
        "\n",
        "\n",
        "Evaluation Criteria\n",
        "Innovation and Approach:\n",
        "Uniqueness and creativity of the proposed solution.\n",
        "Depth of understanding in utilizing advanced deep learning techniques for individualized emotion recognition within groups.\n",
        "\n",
        "\n",
        "Technical Implementation:\n",
        "Clarity and effectiveness of the implementation process.\n",
        "Demonstrated proficiency in integrating and adapting existing technologies to achieve the desired outcomes.\n",
        "\n",
        "\n",
        "Results and Evaluation:\n",
        "Validation of results on some manually labeled images. Here are some group images (3000+) scraped from the Internet.\n",
        "Show some False Positives.\n",
        "\n",
        "\n",
        "Code Quality and Documentation:\n",
        "Readability, efficiency, and organization of the provided code.\n",
        "Completeness and clarity of the accompanying comments and documentation, if any.\n",
        "\n",
        "\n",
        "Challenges and Solutions:\n",
        "Solutions implemented to overcome challenges encountered during the development process.\n",
        "\n",
        "\n",
        "Future Improvements:\n",
        "Thoughtfulness and feasibility of proposed enhancements and future directions for the solution.\n",
        "Try to build a Modular Solution\n",
        "\n",
        "\n",
        "Submission Guidelines\n",
        "Submit the following details in a single PDF and your code in a Python Notebook in a zip file.\n",
        "\n",
        "[PDF] Provide a clear description of the implementation process, including the technologies, frameworks, and libraries.\n",
        "[PDF] Describe how your solution will differentiate individual emotions within groups, highlighting the uniqueness of your approach, if any.\n",
        "Include a step-by-step explanation of how your solution processes group images and individualizes emotion recognition.\n",
        "[PDF] Specify how you integrate and adapt existing technologies to achieve the desired results.\n",
        "[PDF and Colab] Include preliminary results on some manually labeled data, demonstrating the effectiveness of your approach.\n",
        "Present any qualitative and quantitative measures used to evaluate the accuracy and nuance of individual emotion recognition within groups.\n",
        "Showcase comparisons with existing methods (if you explored multiple pre-trained models), emphasizing the improvements achieved by your solution.\n",
        "Provide a well-documented Colab-Notebook.\n",
        "Include clear instructions for running the code, along with any dependencies required.\n",
        "Document the code comprehensively to facilitate understanding and future development.\n",
        "Clearly label each section of your submission to enhance readability.\n",
        "[Optional] Include visualizations, diagrams, or flowcharts illustrating the workflow of your solution. If possible, provide demonstrations or screenshots showcasing your solution in action, highlighting its effectiveness in recognizing individual emotions within groups.\n",
        "[PDF] Challenges Faced and Solutions:\n",
        "Discuss any challenges encountered during the development process and the innovative solutions you implemented to overcome them.\n",
        "Provide insights into potential future enhancements for your solution, addressing any limitations and proposing strategies for improvement.\n",
        "\n",
        "\n",
        "Future Directions(Optional)\n",
        "Applied AI for various applications\n",
        "\n",
        "Multimodal Fusion (Joint Representation Fusion)\n",
        "Explore techniques for effectively fusing information from multiple modalities. (video/images, audio, and text) to improve emotion recognition accuracy.\n",
        "\n",
        "\n",
        "\n",
        "Improving Performance on Video/Images:\n",
        "Combine pre-trained face detectors, individual facial expression recognition models, and super-resolution techniques for more accurate emotion recognition.\n",
        "Real-Time Emotion Feedback: Extend the model to perform live emotion recognition on video streams. Consider implementing real-time emotion feedback mechanisms, where recognized emotions are displayed or communicated back to individuals or the group. This can facilitate immediate awareness and\n",
        "potentially influence group dynamics positively.\n",
        "Emotion Trends and Analysis: Extend the project to analyze trends in emotional states over time. This could involve tracking how emotions change during events, meetings, or group activities, providing insights into emotional dynamics.\n",
        "Audio\n",
        "Incorporate audio analysis by extracting speech embeddings to complement facial expression analysis.\n",
        "Investigate the correlation between facial expressions and speech patterns for improved emotion recognition.\n",
        "Text:\n",
        "Utilize OCRs to detect and analyze text in images/videos (e.g., signs, posters) for inferring emotional content.\n",
        "Enhance text-based emotional cues interpretation through natural language processing and sentiment analysis.\n",
        "\n",
        "\n",
        "Ethical AI Considerations:\n",
        "Research and implement ethical AI practices for responsible data collection, especially in sensitive group settings.\n",
        "Address potential biases in emotion recognition models and develop guidelines for responsible AI deployment in events like rallies and protests."
      ],
      "metadata": {
        "id": "WaFz-AA-nrYh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Face Detection\n",
        "You’ll need to start by accurately detecting individual faces in group images or videos. Here are a few popular techniques:\n",
        "\n",
        "YOLOvX: A variant of the You Only Look Once model optimized for real-time object detection. YOLO is known for its speed and accuracy in detecting objects, and newer versions have improvements that might be suitable for detecting small objects like faces in crowded scenes.\n",
        "Haar Cascades: While less modern and typically less accurate than deep learning methods, Haar Cascades are lightweight and fast, making them suitable for less complex applications or very resource-constrained environments.\n",
        "SSD (Single Shot Detector): This is another excellent choice for real-time detection, providing a good balance between speed and accuracy.\n",
        "Implementation Steps:\n",
        "\n",
        "Choose a detection model based on your specific requirements (speed, accuracy, computational resources).\n",
        "Train or fine-tune the model using a dataset like WIDER FACE, which contains a wide range of face images in various scenarios, or use pre-trained models available in libraries such as OpenCV or TensorFlow.\n",
        "2. Emotion Recognition\n",
        "Once faces are detected, the next step is to classify the emotions of each detected face.\n",
        "\n",
        "DeepFace: This is a deep learning model that is robust for face recognition tasks and can be adapted for emotion recognition.\n",
        "FER Models: Pre-trained models on the Facial Expression Recognition 2013 dataset or similar can classify basic emotions such as happiness, sadness, anger, etc.\n",
        "Implementation Steps:\n",
        "\n",
        "Crop the faces from the group images based on the bounding boxes returned by the face detection model.\n",
        "Pass these cropped face images through the emotion recognition model.\n",
        "Adjust the input size of the images according to the requirements of your chosen emotion recognition model.\n",
        "3. Data Preparation and Manual Labeling\n",
        "Collect and label a set of images manually to train and validate the model. This dataset should be representative of the kinds of group images or videos the model will encounter in deployment.\n",
        "Use these labeled images to adjust the models and fine-tune the parameters.\n",
        "4. Model Evaluation\n",
        "Cross-Entropy Loss: This loss function is suitable for classification tasks like emotion recognition. It measures the performance of a classification model whose output is a probability value between 0 and 1.\n",
        "Sample Analyses: Perform qualitative assessments by visualizing the model's predictions on new images to ensure that it generalizes well across different demographics and environments.\n",
        "Implementation Steps:\n",
        "\n",
        "Use part of your manually labeled dataset for testing to evaluate how well your model performs on unseen data.\n",
        "Calculate accuracy, precision, recall, and F1-score for a more comprehensive evaluation.\n",
        "5. Integration and Deployment\n",
        "Integrate the face detection and emotion recognition models into a single pipeline.\n",
        "Optimize the pipeline for the environment in which it will be deployed, considering factors like computational resources and real-time processing needs.\n",
        "6. Technology Stack Suggestions\n",
        "Python: For overall programming.\n",
        "OpenCV, TensorFlow, PyTorch: For model implementation and operations on images.\n",
        "NumPy, Pandas: For data manipulation.\n",
        "Matplotlib, Seaborn: For data visualization.\n",
        "This framework will require iterative development and testing to fine-tune both the detection and classification components for best performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lHDrOOkWIniG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "D8vXDwUSnxLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install face_recognition\n",
        "!pip install torch torchvision\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deaBmG-kejhi",
        "outputId": "3db2aac4-6a7c-4067-ef4c-a1bfca502d50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting face_recognition\n",
            "  Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting face-recognition-models>=0.3.0 (from face_recognition)\n",
            "  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from face_recognition) (8.1.7)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.10/dist-packages (from face_recognition) (19.24.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from face_recognition) (1.25.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from face_recognition) (9.4.0)\n",
            "Building wheels for collected packages: face-recognition-models\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566170 sha256=c379b22c8112ccea0c74dd8937083a31cb0c2ecf5288a59399280c32ea339827\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/eb/cf/e9eced74122b679557f597bb7c8e4c739cfcac526db1fd523d\n",
            "Successfully built face-recognition-models\n",
            "Installing collected packages: face-recognition-models, face_recognition\n",
            "Successfully installed face-recognition-models-0.3.0 face_recognition-1.3.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import face_recognition\n",
        "import cv2\n",
        "\n",
        "def detect_faces_dl(image):\n",
        "    # Convert the image from BGR (OpenCV format) to RGB (face_recognition format)\n",
        "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Detect faces\n",
        "    face_locations = face_recognition.face_locations(rgb_image)\n",
        "    return face_locations  # Returns a list of tuples of found face locations in css (top, right, bottom, left) order\n"
      ],
      "metadata": {
        "id": "QWRbXtmmhEu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class EmotionCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EmotionCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.act2 = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(64 * 50 * 50, 128)  # Adjust based on your input size and architecture\n",
        "        self.fc2 = nn.Linear(128, 7)  # Assuming 7 emotion classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.act1(self.conv1(x)))\n",
        "        x = self.pool(self.act2(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 50 * 50)  # Flatten the output for the FC layer\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "DmZ1Ny6ahRwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dc9F9aaLnp7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CNILcavjnqQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Load the state dictionary\n",
        "state_dict = torch.load('emotion_detection_model_state.pth')\n",
        "print(state_dict)\n",
        "\n",
        "# Print keys in the state dictionary\n",
        "for key in state_dict.keys():\n",
        "    print(key)\n"
      ],
      "metadata": {
        "id": "38PXugD4r-IJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}